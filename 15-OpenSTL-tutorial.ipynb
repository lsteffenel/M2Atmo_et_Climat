{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/M2Atmo_et_Climat/blob/main/OpenSTL-tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenSTL Tutorial"
      ],
      "metadata": {
        "id": "nZDZQvOEPgbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will provide a comprehensive guide on how to use OpenSTL for your own project.\n",
        "\n",
        "Firstly, we will process video data into `.npy` format for easier storage and access. Next, we will demonstrate how to utilize custom data within OpenSTL. Subsequently, we will use OpenSTL to train and evaluate models. Finally, we will visualize predicted frames and generate `.gif` images or videos.\n",
        "\n",
        "- [0. Setup the environment](#0-setup-the-environment)\n",
        "\n",
        "- [1. Preprocess your data](#1-process-your-data)\n",
        "    - [1.1 Save the dataset](#11-save-the-dataset)\n",
        "    - [1.2 Load the dataset and visualize an example](#12-load-the-dataset-and-visualize-an-example)\n",
        "\n",
        "- [2. Utilize cumtom data](#2-utilize-cumtom-data)\n",
        "    - [2.1 Define the dataset](#21-define-the-dataset)\n",
        "    - [2.2 Get the dataloaders](#22-get-the-dataloaders)\n",
        "\n",
        "- [3. Train and evaluate](#3-train-and-evaluate)\n",
        "    - [3.1 Define the custom configs](#31-define-the-custom-configs)\n",
        "    - [3.2 Setup the experiment](#32-setup-the-experiment)\n",
        "    - [3.3 Start training and evaluation](#33)\n",
        "\n",
        "- [4. Visualization](#4-visualization)\n",
        "    - [4.1 Visualize in a line](#41-visualize-in-a-line)\n",
        "    - [4.2 Generate a GIF](#42-generate-a-gif)\n",
        "\n",
        "Here we go!"
      ],
      "metadata": {
        "id": "F3yqv2vMQNrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setup the environment"
      ],
      "metadata": {
        "id": "GLqzq8LsPoOd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If6abJJoObSH"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/lsteffenel/OpenSTL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/OpenSTL')\n",
        "\n",
        "!python setup.py install\n"
      ],
      "metadata": {
        "id": "SZtb4PoBOmBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lightning lightning-utilities iopath\n",
        "!pip install -q fvcore PyWavelets imageio matplotlib scipy timm torch-optim einops scikit-image\n"
      ],
      "metadata": {
        "id": "RFmipt81UvJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **IMPORTANT NOTE**\n",
        "\n",
        "Due to compatibility issues with the `python setup.py install` command in Colab, OpenSTL cannot be immediately applied to the current environment after installation.\n",
        "\n",
        "Please **restart the session** after installation before running the following code blocks."
      ],
      "metadata": {
        "id": "X-7K7qHodOTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Process your data\n",
        "\n",
        "Assume you possess a collection of videos and have already sorted them into the `train/`, `val/`, and `test/` directories.\n",
        "\n",
        "The existing file hierarchy is as follows:\n",
        "\n",
        "- custom_usage.ipynb\n",
        "- train\n",
        "  - train_example1.avi\n",
        "  - train_example2.avi\n",
        "- val\n",
        "  - val_example1.avi\n",
        "  - val_example2.avi\n",
        "- test\n",
        "  - test_example.avi\n",
        "\n",
        "It's crucial to note that the above example represents a simplified scenario where we've used `.avi` videos for illustrative purposes. However, in a real-world application, you are free to employ videos in various other formats.\n",
        "\n",
        "We utilize several videos from the KTH dataset as examples. Although the original KTH dataset consists of single-channel grayscale videos, we save them as three-channel RGB videos to accommodate a broader range of use cases."
      ],
      "metadata": {
        "id": "a-3Ds3ZNQ3t3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Save the dataset\n",
        "\n",
        "Here, we default to uniformly sampling several frames from the given video data. The number of observed frames and future frames are two hyperparameters (`pre_seq_length`, `aft_seq_length`) that are set in advance. You can also choose to sample the video in other ways and save the data."
      ],
      "metadata": {
        "id": "NPXqnzS9RKhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### hyperparameters\n",
        "\n",
        "`pre_seq_length`: the number of given frames\n",
        "\n",
        "`aft_seq_length`: the number of frames to predict"
      ],
      "metadata": {
        "id": "BUqQek5FRNqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_seq_length = 6\n",
        "aft_seq_length = 6"
      ],
      "metadata": {
        "id": "fd-8GfCCRM68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def sample_frames(video_path, num_frames=20):\n",
        "    # read the video\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(video_path+ \" - \"+ str(total_frames))\n",
        "    # uniformly sample frames from the video\n",
        "    max_idx = total_frames-num_frames\n",
        "    stacks = []\n",
        "    step = 4 # default = 1 ; I put step = 4 just to reduce the dataset size and speedup training\n",
        "    for idx in range(0,max_idx, step):\n",
        "      frames = []\n",
        "      for i in range(num_frames):\n",
        "        video.set(cv2.CAP_PROP_POS_FRAMES, idx+i)\n",
        "        _, frame = video.read()\n",
        "        frames.append(frame)\n",
        "      stack = np.stack(frames).transpose(0, 3, 1, 2)\n",
        "      stacks.append(stack)\n",
        "    video.release()\n",
        "    return stacks\n",
        "\n",
        "def process_folder(folder_path, pre_slen=10, aft_slen=10, suffix='.avi'):\n",
        "    # get all the videos in this folder\n",
        "    videos = []\n",
        "    files = sorted(os.listdir(folder_path))\n",
        "    print(files)\n",
        "    for file in files:\n",
        "        video_path = os.path.join(folder_path, file)\n",
        "        if os.path.isfile(video_path) and file.endswith(suffix):\n",
        "            video = sample_frames(video_path, pre_slen + aft_slen)\n",
        "            videos.extend(video)\n",
        "    # stack video frames from each folder\n",
        "    #data = np.stack(videos).transpose(0, 1, 4, 2, 3)\n",
        "    data = np.array(videos)\n",
        "    print(data.shape)\n",
        "\n",
        "    # if the data is in [0, 255], rescale it into [0, 1]\n",
        "    if data.max() > 1.0:\n",
        "        data = data.astype(np.float32) / 255.0\n",
        "\n",
        "    return data[:, :pre_slen], data[:, pre_slen:]"
      ],
      "metadata": {
        "id": "wnfDrmZXRPgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/OpenSTL')\n",
        "\n",
        "dataset = {}\n",
        "folders = ['train', 'val', 'test']\n",
        "for folder in folders:\n",
        "    data_x, data_y = process_folder('examples/' + folder, pre_slen=pre_seq_length, aft_slen=aft_seq_length, suffix='.avi')\n",
        "    dataset['X_' + folder], dataset['Y_' + folder] = data_x, data_y\n",
        "\n"
      ],
      "metadata": {
        "id": "D5QdpyI4RQmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Load the dataset and visualize an example"
      ],
      "metadata": {
        "id": "yNMqqDnDRT5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = dataset['X_train'], dataset['Y_train']\n",
        "print(train_x.shape)\n",
        "# the shape is B x T x C x H x W\n",
        "# B: the number of samples\n",
        "# T: the number of frames in each sample\n",
        "# C, H, W: the height, width, channel of each frame"
      ],
      "metadata": {
        "id": "pqDszqsaRR20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openstl.utils import show_video_line\n",
        "\n",
        "# show the given frames from an example\n",
        "example_idx = 1\n",
        "show_video_line(train_x[example_idx], ncols=pre_seq_length, vmax=0.6, cbar=False, out_path=None, format='png', use_rgb=True)"
      ],
      "metadata": {
        "id": "wdi1BJFNRgQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the future frames from an example\n",
        "example_idx = 0\n",
        "show_video_line(train_y[example_idx], ncols=aft_seq_length, vmax=0.6, cbar=False, out_path=None, format='png', use_rgb=True)"
      ],
      "metadata": {
        "id": "glDZ-s6pRhVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Utilize cumtom data"
      ],
      "metadata": {
        "id": "BI8HZCssS6dM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Define the dataset\n",
        "\n",
        "We first define the `CustomDataset` to use the provided dataset.\n",
        "\n",
        "For real video prediction, to facilitate model optimization, we strongly recommend users to use frames represented as `float32` with values in the range of $[0, 1]$, rather than frames represented as `uint8` with values in the range of $[0, 255]$. For other types of data, we also provide the option to `normalize` in the `CustomDataset`."
      ],
      "metadata": {
        "id": "rkvI-vVcS9A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, Y, normalize=False, data_name='custom'):\n",
        "        super(CustomDataset, self).__init__()\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "        self.data_name = data_name\n",
        "\n",
        "        if normalize:\n",
        "            # get the mean/std values along the channel dimension\n",
        "            mean = data.mean(axis=(0, 1, 2, 3)).reshape(1, 1, -1, 1, 1)\n",
        "            std = data.std(axis=(0, 1, 2, 3)).reshape(1, 1, -1, 1, 1)\n",
        "            data = (data - mean) / std\n",
        "            self.mean = mean\n",
        "            self.std = std\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = torch.tensor(self.X[index]).float()\n",
        "        labels = torch.tensor(self.Y[index]).float()\n",
        "        return data, labels"
      ],
      "metadata": {
        "id": "-9Lz3cwYRjOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Get the dataloaders\n",
        "\n",
        "Now we can get the dataloaders by using the collected dataset and the defined `CustomDataset`."
      ],
      "metadata": {
        "id": "Q7GlTXS7S_eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### hyperparameters\n",
        "\n",
        "`batch_size`: the size of a batch"
      ],
      "metadata": {
        "id": "8cKOwCulTDSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1"
      ],
      "metadata": {
        "id": "l62rePnqS8EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, Y_train, Y_val, Y_test = dataset['X_train'], dataset[\n",
        "    'X_val'], dataset['X_test'], dataset['Y_train'], dataset['Y_val'], dataset['Y_test']\n",
        "\n",
        "train_set = CustomDataset(X=X_train, Y=Y_train)\n",
        "val_set = CustomDataset(X=X_val, Y=Y_val)\n",
        "test_set = CustomDataset(X=X_test, Y=Y_test)"
      ],
      "metadata": {
        "id": "QSk-zotrTBTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "dataloader_val = torch.utils.data.DataLoader(\n",
        "    val_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=batch_size, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "ootAvhYlTGcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train and evaluate"
      ],
      "metadata": {
        "id": "jlErGIKQTJOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Define the custom configs\n",
        "\n",
        "We first define the training configuration, which includes several important hyperparameters for training.\n",
        "\n",
        "Here, we set `epoch = 3` as an example for simple training. The actual value of epoch in your project depends on the complexity of your dataset. Generally, a good starting point is `epoch = 100`.\n",
        "\n",
        "Next, we define the model configuration to customize a spatio-temporal predictive learning model. For MetaVP models, the key hyperparameters are: `N_S`, `N_T`, `hid_S`, `hid_T`, and `model_type`. Users have the option to either use a config file or directly set these hyperparameters.\n",
        "\n",
        "If a config file is preferred, the user can simply include a `config_file` key with the path to the config file in the `custom_model_config` below. Otherwise, the user can directly specify these hyperparameters in the `custom_model_config`."
      ],
      "metadata": {
        "id": "uqS2nQf8TKja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_training_config = {\n",
        "    'pre_seq_length': pre_seq_length,\n",
        "    'aft_seq_length': aft_seq_length,\n",
        "    'total_length': pre_seq_length + aft_seq_length,\n",
        "    'batch_size': batch_size,\n",
        "    'val_batch_size': batch_size,\n",
        "    'epoch': 25,\n",
        "    'lr': 0.005,\n",
        "    'metrics': ['mse', 'mae'],\n",
        "    'ex_name': 'custom_exp',\n",
        "    'dataname': 'custom',\n",
        "    'in_shape': [6, 3, 180,320], # 6 images (input), 3 colours, height, width\n",
        "}\n",
        "\n",
        "custom_model_config = {\n",
        "    # For MetaVP models, the most important hyperparameters are:\n",
        "    # N_S, N_T, hid_S, hid_T, model_type\n",
        "    'method': 'SimVP',\n",
        "    # Users can either using a config file or directly set these hyperparameters\n",
        "    # 'config_file': 'configs/custom/example_model.py',\n",
        "\n",
        "    # Here, we directly set these parameters\n",
        "    'model_type': 'gSTA',\n",
        "    'N_S': 2,\n",
        "    'N_T': 6,\n",
        "    'hid_S': 32,\n",
        "    'hid_T': 128\n",
        "}"
      ],
      "metadata": {
        "id": "rr-O9kXVTH-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Setup the experiment\n",
        "\n",
        "We retrieve the default hyperparameters by utilizing `create_parser` and update those hyperparameters that are defined in `custom_training_config` and `custom_model_config`.\n",
        "\n",
        "By utilizing `BaseExperiment`, we ensure that everything is prepared, including dataloader setup and model initialization."
      ],
      "metadata": {
        "id": "VOhtu-ACTMm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openstl.api import BaseExperiment\n",
        "from openstl.utils import create_parser, default_parser\n",
        "\n",
        "args = create_parser().parse_args([])\n",
        "config = args.__dict__\n",
        "\n",
        "# update default parameters\n",
        "default_values = default_parser()\n",
        "for attribute in default_values.keys():\n",
        "    if config[attribute] is None:\n",
        "        config[attribute] = default_values[attribute]\n",
        "\n",
        "# update the training config\n",
        "config.update(custom_training_config)\n",
        "# update the model config\n",
        "config.update(custom_model_config)\n",
        "\n",
        "exp = BaseExperiment(args, dataloaders=(dataloader_train, dataloader_val, dataloader_test), strategy='auto')"
      ],
      "metadata": {
        "id": "DgKIUjFmTLuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Start training and evaluation\n",
        "\n",
        "With everything in place, we are now ready to start training and evaluation.\n",
        "\n",
        "Here we go!"
      ],
      "metadata": {
        "id": "Eo6hZPkXTSlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('>'*35 + ' training ' + '<'*35)\n",
        "exp.train()\n",
        "\n",
        "print('>'*35 + ' testing  ' + '<'*35)\n",
        "exp.test()"
      ],
      "metadata": {
        "id": "J2wvjtrJTOt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "\n",
        "By employing the `show_video_line` and `show_video_gif_multiple` functions provided by `OpenSTL`, we can visualize the input, ground truth, and predicted frames, as well as generate corresponding GIFs."
      ],
      "metadata": {
        "id": "DJa-q9SWTT9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Visualize in a line"
      ],
      "metadata": {
        "id": "4xyoEv2NTV4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from openstl.utils import show_video_line\n",
        "\n",
        "# show the given frames from an example\n",
        "inputs = np.load('./work_dirs/custom_exp/saved/inputs.npy')\n",
        "preds = np.load('./work_dirs/custom_exp/saved/preds.npy')\n",
        "trues = np.load('./work_dirs/custom_exp/saved/trues.npy')\n",
        "\n",
        "example_idx = 15\n",
        "show_video_line(trues[example_idx], ncols=aft_seq_length, vmax=0.6, cbar=False, out_path=None, format='png', use_rgb=True)"
      ],
      "metadata": {
        "id": "5nMv9o2FTVA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (inputs.shape)"
      ],
      "metadata": {
        "id": "ZpIsCLzMdGAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "show_video_line(preds[example_idx], ncols=aft_seq_length, vmax=0.6, cbar=False, out_path=None, format='png', use_rgb=True)"
      ],
      "metadata": {
        "id": "lvxpA1lBTYoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Generate a GIF"
      ],
      "metadata": {
        "id": "XUva05kwTa0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openstl.utils import show_video_gif_multiple\n",
        "\n",
        "show_video_gif_multiple(inputs[example_idx], trues[example_idx], preds[example_idx], use_rgb=True, out_path='example.gif')"
      ],
      "metadata": {
        "id": "GgwKEr3LTXfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVhFfpJ5rUnl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
